{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python souce를 parsing하고 변수명을 추출한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sample : tensorflow (https://github.com/tensorflow/tensorflow)\n",
    "* 참고 : https://www.python.org/dev/peps/pep-0008/#naming-conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "import re \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* source down load list \n",
    "<pre>\n",
    "git clone https://github.com/tensorflow/tensorflow\n",
    "git clone https://github.com/django/django.git\n",
    "git clone https://github.com/scikit-learn/scikit-learn.git\n",
    "git clone https://github.com/pallets/flask.git\n",
    "git clone https://github.com/ansible/ansible.git\n",
    "git clone https://github.com/odoo/odoo.git\n",
    "git clone https://github.com/scrapy/scrapy.git\n",
    "git clone https://github.com/rg3/youtube-dl.git\n",
    "git clone https://github.com/kennethreitz/requests.git\n",
    "git clone https://github.com/tornadoweb/tornado.git\n",
    "git clone https://github.com/fchollet/keras.git\n",
    "git clone https://github.com/ipython/ipython.git\n",
    "git clone https://github.com/pandas-dev/pandas.git\n",
    "git clone https://github.com/numpy/numpy.git\n",
    "git clone https://github.com/matplotlib/matplotlib.git\n",
    "git clone https://github.com/mwaskom/seaborn.git\n",
    "git clone https://github.com/networkx/networkx.git\n",
    "git clone https://github.com/scipy/scipy.git\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readSource(path):\n",
    "    try :\n",
    "        with open(path) as f:\n",
    "            content = f.read()\n",
    "        return content\n",
    "    except :\n",
    "        return 'file exception'\n",
    "            \n",
    "    return ''\n",
    "\n",
    "\n",
    "def removeComments(source):\n",
    "    \"\"\"\n",
    "    replacde \"\"\"\"\"\" and '#' style comments\n",
    "    reference : http://blog.ostermiller.org/find-comment \n",
    "    comment의 열림만 있고 닫힘이 없는 경우 regex에서 hang이 걸리는 경우가 발생함 \n",
    "    이름 방지하기 위해 사용 \n",
    "    \"\"\"\n",
    "    source += '*/'\n",
    "\n",
    "    # java comment\n",
    "    # p = re.compile('(\\/\\*([^*]|[\\r\\n]|(\\*([^/]|[\\r\\n])))*\\*\\/)|(\\/\\/.*)')\n",
    "    # p = re.compile('(#.*)|(\\\"((?:.|\\n)*?)\\\")')\n",
    "    p = re.compile('(#.*)|(\\\"(.|\\n)*?\\\")')\n",
    "    \n",
    "    \n",
    "    output = p.sub(\"\", source)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def cleaningSource(source):\n",
    "    ## remove ccomments \n",
    "    source = removeComments(source)\n",
    "    \n",
    "    ## remove \"\\r\"\n",
    "    source = source.replace(\"\\r\",\"\")\n",
    "    \n",
    "    ## split by lines\n",
    "    source_lines = source.split(\"\\n\")\n",
    "    \n",
    "    return source_lines\n",
    "\n",
    "def parseSourceLines(lines):\n",
    "    variable_names = []\n",
    "    variable_set = set()\n",
    "    for line in lines:\n",
    "        vals = parseLine(line)\n",
    "        if  (vals==None): \n",
    "            continue\n",
    "        \n",
    "        for val in vals :\n",
    "            ## unique check\n",
    "            if val[0] not in variable_set:\n",
    "                variable_names.append(val)\n",
    "                variable_set.add(val[0])\n",
    "    return variable_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## test code \n",
    "\n",
    "sample_path = \"/Users/goodvc/Data/naming-recsys/resource/python/tensorflow/tensorflow/python/framework/common_shapes.py\"\n",
    "sample_src = readSource(sample_path)\n",
    "\n",
    "cleaningSource(sample_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(sample_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reserved_words = set('and del from not while as elif global or with assert else if pass yield break except default abstract import print class exec in raise continue finally is return def for lambda try '.split())     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Java 예약어 \n",
    "#reserved_words = set('abstract default package synchronized boolean do if private this break double implements protected throw byte else import public throws switch enum instanceof return try catch extends int short char final interface static void class finally long strictfp volatile float native super while continue for new case goto* null transient const operator future generic ineer outer rest var from'.split())\n",
    "\n",
    "def variableValidation(name):\n",
    "    name = name.lower()\n",
    "    ## check reserved words \n",
    "    if name in reserved_words:\n",
    "        return False\n",
    "    \n",
    "    ## check start-char is number\n",
    "    if name[0].isnumeric():\n",
    "        return False\n",
    "    \n",
    "    ## check test code's name\n",
    "    if name.find(\"test\") > -1:\n",
    "        return False\n",
    "    \n",
    "    ## __{keyword}__ pattern \n",
    "    if name[:2]=='__' and name[-2:]=='__':\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Variable Extract Rule ###############################\n",
    "\n",
    "def ev_equal_rule(line):\n",
    "    \"\"\"\n",
    "    변수는 변할수 있는 값이라는 전제로 값을 변하게 하는 equal(=) 연산자의 left token을 변수를 함 \n",
    "    \"\"\"\n",
    "    line = line.replace('==',' ')\n",
    "    equal_pos = line.find('=')\n",
    "    if equal_pos < 0:\n",
    "        return None\n",
    "    line = ''.join( c if (c.isalnum()) | (c=='_') else ' ' for c in line[:equal_pos] )\n",
    "    val = line.split()[-1]\n",
    "    if len(val)>0:\n",
    "        return [val]\n",
    "    return None\n",
    "\n",
    "def ev_equal_regex_rule(line):\n",
    "    \"\"\" \n",
    "    '=' 이전에 공백이나 탭이 0~3개 까지 올수 있고 \n",
    "    '=' 이전에 A-Z|a-z|0-9|_ 로 구성된 문자가 오고 \n",
    "    '=' 다음에 '<>!' 문자가 안오는 경우 \n",
    "    \"\"\"\n",
    "    regex = r\"([A-Za-z0-9\\_]+)[ \\t]{0,3}\\=[^<>!]\"\n",
    "    line += ' '\n",
    "    matches = re.finditer(regex, line)\n",
    "    names = []\n",
    "    for match in matches:\n",
    "        names.append(match.group(1))\n",
    "    return names\n",
    "\n",
    "\n",
    "##### Class Extract Rule ###############################\n",
    "def ec_prefix_regex_rule(line):\n",
    "    \"\"\"\n",
    "    'class' 단어가 오고 \n",
    "    ' '이 1글자 이상오고  \n",
    "    다음에 a-z, A-Z, _, 0-9 글자로 이루어진 단어를 class명으로 추출 \n",
    "    \"\"\"\n",
    "    regex = r\"(class) {1,3}([a-zA-Z_0-9]+)\"\n",
    "    line += ' '\n",
    "    matches = re.finditer(regex, line)\n",
    "    names = []\n",
    "    for match in matches:\n",
    "        names.append(match.group(2))\n",
    "        \n",
    "    return names \n",
    "\n",
    "def ev_regex_rule(line, regex, groupid):\n",
    "    line += ' '\n",
    "    matches = re.finditer(regex, line)\n",
    "    names = []\n",
    "    for match in matches:\n",
    "        names.append(match.group(groupid))\n",
    "    return names \n",
    "\n",
    "## 함수명을 추출하는 rule, 단순히 '('가 후미에 있고 단어가 a-z, A-Z, 0-9, _ 구성됨\n",
    "#ef_bracket_regex_rule = lambda line : ev_regex_rule(line, r' {1,3}([a-zA-Z0-9]+) {0,3}\\(', 1)\n",
    "ef_def_regex_rule = lambda line : ev_regex_rule(line, r'(def) {1,3}([a-zA-Z_0-9]+)', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ef_braket_regex_rule(line):\n",
    "    \"\"\"\n",
    "    단순히 '('가 후미에 있고 단어가 a-z, A-Z, 0-9, _ 구성됨\n",
    "    \"\"\"\n",
    "    regex = r\"{1,3}([a-zA-Z0-9]+) {0,3}\\(\"\n",
    "    line += ' '\n",
    "    matches = re.finditer(regex, line)\n",
    "    names = []\n",
    "    for match in matches:\n",
    "        names.append(match.group(1))\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "def extractVariable(line):\n",
    "    ## parse rule list \n",
    "    parse_rules = [\n",
    "        ('equal:rule', 'variable', ev_equal_regex_rule), ## equal rule by regex \n",
    "        ('class:rule', 'class', ec_prefix_regex_rule), ##\n",
    "        ('function:rule', 'function', ef_def_regex_rule), ##\n",
    "    ]\n",
    "    \n",
    "    val_names = []\n",
    "    for (rule, val_type, parsor) in parse_rules:\n",
    "        names = parsor(line)\n",
    "        if (names == None):\n",
    "            continue\n",
    "        for name in names:\n",
    "            val_names.append([name,val_type]) \n",
    "    \n",
    "    return val_names\n",
    "\n",
    "def extractIntent(line):\n",
    "    intent_char = ' \\t'\n",
    "    pos = 0\n",
    "    for ch in line:\n",
    "        if ch not in intent_char:\n",
    "            break\n",
    "        pos = pos + (4 if ch == '\\t' else 1)\n",
    "    return pos\n",
    "\n",
    "def parseLine(line):\n",
    "    ## check intent\n",
    "    intent = extractIntent(line)\n",
    "    \n",
    "    ## extract variables \n",
    "    vals = extractVariable(line)\n",
    "    if (vals == None) | (len(vals) == 0): \n",
    "        return None\n",
    "    \n",
    "    ret = []\n",
    "    for val in vals:\n",
    "        ## check name validation\n",
    "        if False==variableValidation(val[0]):\n",
    "            continue\n",
    "        ## add intent value \n",
    "        val.append(intent)\n",
    "        ret.append(val)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def parseSourceCode(src_path):\n",
    "    source = readSource(src_path)\n",
    "    source = cleaningSource(source)\n",
    "    parsed = parseSourceLines(source)\n",
    "    \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## test code \n",
    "print(sample_path)\n",
    "parseSourceCode(sample_path)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import os\n",
    "\n",
    "def findFiles(path, pattern):\n",
    "    matches = []\n",
    "    for root, dirnames, filenames in os.walk(path):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            matches.append(os.path.join(root, filename))\n",
    "    return matches\n",
    "\n",
    "def parseSourceDir(filenames):\n",
    "    pared_variables = []\n",
    "\n",
    "    for path in filenames:\n",
    "        pared_variables.extend(parseSourceCode(path))\n",
    "    return pared_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def charType(ch):\n",
    "    if ch=='_':\n",
    "        return 'underbar'\n",
    "    \n",
    "    if ch.isnumeric():\n",
    "        return 'numeric'\n",
    "    \n",
    "    if not ch.isalpha():\n",
    "        return 'unknown'\n",
    "    \n",
    "    if ch.islower():\n",
    "        return 'lower'\n",
    "    else :\n",
    "        return 'upper'\n",
    "\n",
    "    return 'unknown'\n",
    "\n",
    "def chLevel(ch):\n",
    "    if ch.islower():   return 1\n",
    "    if ch.isupper():   return 2\n",
    "    if ch.isnumeric(): return 3\n",
    "    if ch=='_':        return 4\n",
    "    if ch=='$':        return 0\n",
    "    return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parse', 'DBMXML', 'From', 'IP', 'Address']\n"
     ]
    }
   ],
   "source": [
    "def removeNumber(word):\n",
    "    removed = ''\n",
    "    for ch in word:\n",
    "        if not ch.isnumeric():\n",
    "            removed += ch\n",
    "    return removed\n",
    "\n",
    "\n",
    "## https://en.wikipedia.org/wiki/Naming_convention_(programming)#Java \n",
    "def tokenizer(sentance):\n",
    "    \"\"\" \n",
    "    naming된 sentance를 단어로 tokenizing 한다. \n",
    "    java naming convention에 기반하여 check\n",
    "    UpperCamelCase, lowerCamelCase, lower_delimiter_case, UPPER_DELIMITER_CASE \n",
    "    위의 4가지 naming convention 으로 tokenize 실행 \n",
    "    \"\"\"\n",
    "    ### inspection split char posision \n",
    "    old_level = 0\n",
    "    split_pos = []\n",
    "    last_pos = 0\n",
    "    for pos, ch in enumerate(sentance):\n",
    "        cur_level = chLevel(ch)\n",
    "        if (cur_level < old_level) & (pos>1) :  ## lower edge\n",
    "            if old_level == 3: \n",
    "                split_pos.append(last_pos)\n",
    "            elif (pos - last_pos)<2:\n",
    "                split_pos.append(last_pos)\n",
    "            else :\n",
    "                split_pos.extend([last_pos,pos-1])\n",
    "            last_pos = pos\n",
    "        elif ( cur_level > old_level ): ## upper edge\n",
    "            #print('set')\n",
    "            last_pos = pos\n",
    "        old_level = cur_level\n",
    "        \n",
    "\n",
    "    ### word split \n",
    "    last_pos = 0\n",
    "    words = []\n",
    "    split_pos.append(pos+1)\n",
    "    for pos in split_pos:\n",
    "        if sentance[last_pos]=='_':\n",
    "            last_pos += 1\n",
    "        w = removeNumber(sentance[last_pos:pos])\n",
    "        if len(w)>0:\n",
    "            words.append( w )\n",
    "        last_pos=pos\n",
    "    return words \n",
    "\n",
    "\n",
    "print(tokenizer('parseDBMXMLFromIPAddress'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test100 ['test']\n",
      "tokenStats ['token', 'Stats']\n",
      "ActiveMQQueueMarshaller ['Active', 'MQ', 'Queue', 'Marshaller']\n",
      "parseDBMXMLFromIPAddress ['parse', 'DBMXML', 'From', 'IP', 'Address']\n",
      "TestMapFile ['Test', 'Map', 'File']\n",
      "TEST_NUMVER_AA ['TEST', 'NUMVER', 'AA']\n",
      "my_number ['my', 'number']\n",
      "removeNumber ['remove', 'Number']\n",
      "split_pos ['split', 'pos']\n"
     ]
    }
   ],
   "source": [
    "testSentance = ['test100', 'tokenStats', 'ActiveMQQueueMarshaller', \n",
    "                'parseDBMXMLFromIPAddress', 'TestMapFile', 'TEST_NUMVER_AA',\n",
    "                'my_number','removeNumber', 'split_pos' ]\n",
    "for word in testSentance:\n",
    "    print(word,tokenizer(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in testSentance:\n",
    "    print('^'+word)\n",
    "    print(''.join(['0']+[ str(chLevel(ch)) for ch in word]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 소스 데이터 추출 \n",
    "* 01-2. extract-naming-words-in-python-source.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readSource\n",
      "removeComments\n",
      "cleaningSource\n",
      "parseSourceLines\n",
      "variableValidation\n",
      "ev_equal_rule\n",
      "ev_equal_regex_rule\n",
      "ec_prefix_regex_rule\n",
      "ev_regex_rule\n",
      "ef_braket_regex_rule\n",
      "extractVariable\n",
      "extractIntent\n",
      "parseLine\n",
      "parseSourceCode\n",
      "findFiles\n",
      "parseSourceDir\n",
      "charType\n",
      "chLevel\n",
      "removeNumber\n",
      "tokenizer\n",
      "checkDirAndParse\n",
      "tokenizeList\n"
     ]
    }
   ],
   "source": [
    "cur_source_ds = pd.DataFrame(parseSourceCode(\"./01-2. extract-naming-words-in-python-source.py\"))\n",
    "for idx,name in cur_source_ds[cur_source_ds[1]=='function'][[0]].iterrows():\n",
    "    print(name[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 수집 및 랭클링 작업 \n",
    "* github에서 popular fork repo download  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variable_data = {}\n",
    "variable_meta = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "\n",
    "home_dir = \"/Users/goodvc/Data/naming-recsys/resource/source/python\"\n",
    "            \n",
    "\n",
    "def checkDirAndParse(output_data, ouput_meta):\n",
    "    ## check dir \n",
    "    base_dir = home_dir\n",
    "    folders = [f for f in listdir(base_dir) if isdir(join(base_dir, f))]\n",
    "    \n",
    "    for topic in folders:\n",
    "        if topic in output_data:\n",
    "            continue\n",
    "        filenames = findFiles( os.path.join(home_dir, topic) , '*.py')\n",
    "        print(\" %s topic %d files parsing start\" % (topic, len(filenames)))\n",
    "        ouput_meta[topic] = {'file_count':len(filenames)}\n",
    "        output_data[topic] = parseSourceDir(filenames)\n",
    "    print(\"parse end\")\n",
    "\n",
    "## \n",
    "checkDirAndParse(variable_data, variable_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## parsing 한 repository 기본 정보 \n",
    "repo_meta_ds = pd.DataFrame.from_dict(variable_meta, orient='index').reset_index()\n",
    "repo_meta_ds.columns = 'topic file_cnt'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 변수 Tokenize \n",
    "def tokenizeList(data_list):\n",
    "    token_list = []\n",
    "    for val in data_list:\n",
    "        tokens = tokenizer(val[0])\n",
    "        token_list.extend([[token, token.lower(),val[1], val[3]] for token in tokens if len(token)>0 ])\n",
    "    return token_list\n",
    "## 1207277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## variable data merge\n",
    "data_list = []\n",
    "for topic in variable_data.keys():\n",
    "    for info in variable_data[topic]:\n",
    "        data_list.append( info + [topic] )\n",
    "\n",
    "## variable data to dataframe \n",
    "name_ds = pd.DataFrame(data_list, columns=['name', 'kind', 'intent', 'topic'])\n",
    "\n",
    "## 단어단위로 분리하고 데이터 셋 만들기 \n",
    "naming_words = tokenizeList(data_list)\n",
    "words_ds = pd.DataFrame(naming_words, columns=['word', 'lower', 'kind', 'topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_name_ds = name_ds.groupby(['name', 'kind']).count().reset_index()[['name','kind','intent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('get', 'VB'), ('compressor', 'NN'), ('type', 'NN')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokens = nltk.word_tokenize('get compressor type')\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "## POS Tagging Dataframe 생성\n",
    "analyzered = []\n",
    "for (idx,row) in unique_name_ds.iterrows():\n",
    "    word = row['name']\n",
    "    tokens = tokenizer(word)\n",
    "    tokens = [ token.lower() for token in tokens ]\n",
    "    tagged = \"+\".join([ pos for (w,pos) in nltk.pos_tag(tokens) ])\n",
    "    analyzered.append([len(tokens), tokens, tagged, row['intent']])\n",
    "\n",
    "pos_tagged_ds = pd.DataFrame(analyzered, columns=['len','tokens', 'pos', 'count'])\n",
    "pos_tagged_ds['kind'] = unique_name_ds.kind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_type='python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 데이터셋 저장 \n",
    "name_ds.to_pickle('./resource/{type}_name_ds.pkl'.format(type=source_type))\n",
    "words_ds.to_pickle('./resource/{type}_words_ds.pkl'.format(type=source_type))\n",
    "repo_meta_ds.to_pickle('./resource/{type}_repo_ds.pkl'.format(type=source_type))\n",
    "pos_tagged_ds.to_pickle('./resource/{type}_pos_tagged_ds.pkl'.format(type=source_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## sample 테스트 \n",
    "words = tokenizer('parseDBMXMLFromIPAddress')\n",
    "words = [w.lower() for w in words]\n",
    "print( [ \"%s[%s]\" %(w,pos) for (w,pos) in nltk.pos_tag(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추출된 네이밍 데이터 셋 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_ds[102000:102005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tagged_ds[3000:3005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
